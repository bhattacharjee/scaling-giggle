{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtKLvr40DAFTSKoM32NcrA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/scaling-giggle/blob/main/parse_electoral_roll.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Installing the dependencies first\n",
        "\n",
        "We rely on two packages mainly, pdf2image and pytesseract"
      ],
      "metadata": {
        "id": "4lDfdI34O-j6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6s6GED3bOyAO"
      },
      "outputs": [],
      "source": [
        "# Install the dependencies\n",
        "!pip install -q pdf2image\n",
        "!pip install -q pytesseract\n",
        "!pip install -q wget\n",
        "!pip install -q \"tqdm>=4.36.1\"\n",
        "\n",
        "!apt-get install poppler-utils                      > /dev/null 2>&1\n",
        "!apt-get install libleptonica-dev                   > /dev/null 2>&1\n",
        "!apt-get install tesseract-ocr tesseract-ocr-dev    > /dev/null 2>&1\n",
        "!apt-get install libtesseract-dev                   > /dev/null 2>&1\n",
        "!apt-get install tesseract-ocr                      > /dev/null 2>&1\n",
        "!apt-get install tesseract-ocr-eng                  > /dev/null 2>&1\n",
        "!apt-get install tesseract-ocr-eng                  > /dev/null 2>&1\n",
        "\n",
        "import os\n",
        "import re\n",
        "import wget\n",
        "import json\n",
        "import tqdm\n",
        "import shutil\n",
        "import tempfile\n",
        "import logging\n",
        "import pdf2image\n",
        "import pytesseract\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import lru_cache\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " class Roll:\n",
        "    def __init__(self, url:str, save_directory:str=None)->list:\n",
        "        \"\"\"Construct the object which will be used for further\n",
        "        processing\n",
        "\n",
        "        Parameters:\n",
        "        url (str): The URL to the PDF (should not be a redirect)\n",
        "        \"\"\"\n",
        "\n",
        "        self.temp_file_name = None\n",
        "        self.pdf_url = url\n",
        "        self.pages = None\n",
        "        self.pages_text = list()\n",
        "        self.voters = list()\n",
        "\n",
        "        self.town = \"UNKNOWN\"\n",
        "        self.block = \"UNKNOWN\"\n",
        "        self.post_office = \"UNKNOWN\"\n",
        "        self.police_station = \"UNKNOWN\"\n",
        "        self.pin_code = \"000000\"\n",
        "        self.save_filename = None\n",
        "        self.savedir = save_directory\n",
        "        self.processed_df = None\n",
        "\n",
        "        self.errors = \"\"\n",
        "\n",
        "        self.save_filename = f\"{url.split('/')[-1]}.csv\"\n",
        "\n",
        "        self.re_other_name = re.compile(\\\n",
        "            \"((other.?s|father.?s|mother.?s|husband.?s)\\s?name\\s*[=:>-])\",\\\n",
        "            re.IGNORECASE)\n",
        "        self.re_other_name_for_match = re.compile(\\\n",
        "            \"((other.?s|father.?s|mother.?s|husband.?s)\\s?name\\s*[=:>-])\",\\\n",
        "            re.IGNORECASE)\n",
        "\n",
        "        self.re_name = re.compile(\"(name\\s*[=:>-])\", re.IGNORECASE)\n",
        "        self.re_name_for_match = re.compile(\".*(name\\s*[=:>-])\", re.IGNORECASE)\n",
        "\n",
        "        self.re_house_num = re.compile(\n",
        "            \"(House\\s*number\\s*[:=>-]\\s*)\", re.IGNORECASE)\n",
        "        self.re_house_num_for_match = re.compile(\n",
        "            \".*(House\\s*number\\s*[:=>-]\\s*)\", re.IGNORECASE)\n",
        "\n",
        "        self.re_age_gender = re.compile(\n",
        "            \"(age\\s*[:=>-]\\s*(\\d+)\\s*gender\\s*[:=>-]\\s*(male|female))\",\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "        self.re_age_gender_for_match = re.compile(\n",
        "            \".*(age\\s*[:=>-]\\s*(\\d+)\\s*gender\\s*[:=>-]\\s*(male|female))\",\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "\n",
        "        self.re_age = re.compile(\"(age\\s*[:=>-]\\s*(\\d*))\", re.IGNORECASE)\n",
        "        self.re_age_for_match = re.compile(\\\n",
        "                \".*(age\\s*[:=>-]\\s*(\\d*))\", re.IGNORECASE\n",
        "        )\n",
        "\n",
        "\n",
        "        self.page_details = {}\n",
        "\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
        "        self.temp_file_name = f\"{temp_file.name}.pdf\"\n",
        "        temp_file.close()\n",
        "    \n",
        "\n",
        "    @lru_cache(maxsize=256)\n",
        "    def get_text_as_list(self, text):\n",
        "        text = [s.strip() for s in text.split('\\n')]\n",
        "        text = [s for s in text if len(s) > 0]\n",
        "        names = list()\n",
        "        gender = list()\n",
        "        other = list()\n",
        "        return text\n",
        "\n",
        "    def download(self)->None:\n",
        "        \"\"\"Download the PDF file for this object\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        wget.download(self.pdf_url, self.temp_file_name)\n",
        "        if not os.path.isfile(self.temp_file_name) or \\\n",
        "            0 == os.stat(self.temp_file_name).st_size:\n",
        "            raise Exception(\"Failed to download file\")\n",
        "\n",
        "    def parse_first_page(self)->None:\n",
        "        \"\"\"\n",
        "        First page contains a lot of details, parse them\n",
        "        to fill the details of the geolocation of electoral roll\n",
        "        \"\"\"\n",
        "        re_town_village = re.compile(\".*town.*village\\s*[=:]\\s*(.*)\", \\\n",
        "                                    re.IGNORECASE)\n",
        "        re_post_office = re.compile(\".*Post.*Office\\s*[=:]\\s*(.*)\", \\\n",
        "                                    re.IGNORECASE)\n",
        "        re_pin_code = re.compile(\".*pin.*code.*\\s*([0-9]{6})\\s*\",\n",
        "                                    re.IGNORECASE)\n",
        "        re_block = re.compile(\".*block\\s[=:]\\s*(.*)\", re.IGNORECASE)\n",
        "        re_district = re.compile(\".*district\\s:\\s*(.*)\", re.IGNORECASE)\n",
        "        re_police_st = re.compile(\".*police.*station\\s*[=:]\\s*(.*)\",\\\n",
        "                                    re.IGNORECASE)\n",
        "        text = self.get_text_as_list(self.pages_text[0])\n",
        "        for s in text:\n",
        "            m = re_town_village.match(s)\n",
        "            if m:\n",
        "                self.town = m.group(1).strip()\n",
        "                continue\n",
        "            m = re_post_office.match(s)\n",
        "            if m:\n",
        "                self.post_office = m.group(1).strip()\n",
        "                continue\n",
        "            m = re_pin_code.match(s)\n",
        "            if m:\n",
        "                self.pin_code = m.group(1).strip()\n",
        "                continue\n",
        "            m = re_block.match(s)\n",
        "            if m:\n",
        "                self.block = m.group(1).strip()\n",
        "                continue\n",
        "            m = re_district.match(s)\n",
        "            if m:\n",
        "                self.district = m.group(1).strip()\n",
        "                continue\n",
        "            m = re_police_st.match(s)\n",
        "            if m:\n",
        "                self.police_station = m.group(1).strip()\n",
        "                continue\n",
        "                \n",
        "    def convert_to_text(self, i:int)->None:\n",
        "        \"\"\"\n",
        "        Convert an image to text using pytesseract.\n",
        "        Pages from the PDF have already been converted to images\n",
        "        and stored in a dictionary indexed by page number\n",
        "        \"\"\"\n",
        "        s = pytesseract.image_to_string(self.pages[i])\n",
        "        s = s.replace(\"Age:\", \"\\r\\nAge:\")\n",
        "        s = s.replace(\"Photo is\", \"\\r\\nPhoto is\")\n",
        "        #s = s.replace(\"|\" , \"\\r\\n\")\n",
        "        #s = s.replace(\"[\", \"\\r\\n\")\n",
        "        #s = s.replace(\"]\", \"\\r\\n\")\n",
        "        return s\n",
        "\n",
        "    def get_other_name(self, s:str)->list:\n",
        "        \"\"\"\n",
        "        There can be several names in a single line as follows:\n",
        "        Fathers's Name: LAMJINGKMEN KHONGBUH Fathers' Name = LEM! \n",
        "                            CHALLAM Father's Name = PRECIOUSLY RYNGKHLEM\n",
        "        These need to be split and returned as a list\n",
        "        \"\"\"\n",
        "        matches = self.re_other_name.findall(s)\n",
        "        for a, b in matches:\n",
        "            s = s.replace(a, \"|\")\n",
        "        names = [x.strip() for x in s.split(\"|\")]\n",
        "        names = [x for x in names if len(x) > 0]\n",
        "        return names\n",
        "    \n",
        "    def get_name(self, s:str)->list:\n",
        "        \"\"\"\n",
        "        Do the same things for namess other's names\n",
        "        \"\"\"\n",
        "        matches = self.re_name.findall(s)\n",
        "        for a in matches:\n",
        "            s = s.replace(a, \"|\")\n",
        "        names = [x.strip() for x in s.split(\"|\")]\n",
        "        names = [x for x in names if len(x) > 0]\n",
        "        return names\n",
        "\n",
        "    def get_house_num(self, s:str)->list:\n",
        "        \"\"\"\n",
        "        Do the same thing for house number\n",
        "        \"\"\"\n",
        "        matches = self.re_house_num.findall(s)\n",
        "        for a in matches:\n",
        "            s = s.replace(a, \"|\")\n",
        "        names = [x.strip() for x in s.split(\"|\")]\n",
        "        names = [x for x in names if len(x) > 0]\n",
        "        return names\n",
        "\n",
        "    def get_age_gender(self, s:str)->tuple:\n",
        "        \"\"\"\n",
        "        Do the same thing for age and gender.\n",
        "        Age and gender appear in the same line.\n",
        "\n",
        "        This funciton matches lines that contain both age and gender\n",
        "        \n",
        "        There may be cases where lines contain only\n",
        "        age or only gender\n",
        "\n",
        "        Those are handled by get_age_only, and get_gender_only\n",
        "        \"\"\"\n",
        "        matches = self.re_age_gender.findall(s)\n",
        "        ages = list()\n",
        "        genders = list()\n",
        "        for _, age, gender in matches:\n",
        "            ages.append(age)\n",
        "            genders.append(gender)\n",
        "        return ages, genders\n",
        "\n",
        "    def get_age_only(self, s:str)->list:\n",
        "        \"\"\"\n",
        "        Do the same for age. Match lines that contain only age but not gender\n",
        "        \"\"\"\n",
        "        matches = self.re_age.findall(s)\n",
        "        ages = list()\n",
        "        assert(False)\n",
        "        return []\n",
        "\n",
        "    def get_temp_file_name(self)->str:\n",
        "        temp_file = tempfile.NamedTemporaryFile()\n",
        "        temp_file_name = f\"{temp_file.name}.json\"\n",
        "        temp_file.close()\n",
        "        return temp_file_name\n",
        "\n",
        "\n",
        "    def parse_roll_page(self, pagenum:int)->dict:\n",
        "        page_other_names = []\n",
        "        page_names = []\n",
        "        page_house_numbers = []\n",
        "        page_genders = []\n",
        "        page_ages = []\n",
        "\n",
        "        if not pagenum in self.pages_text:\n",
        "            raise Exception(\"page not found\")\n",
        "        text = self.get_text_as_list(self.pages_text[pagenum])\n",
        "        for s in text:\n",
        "            # Must match other name first, becuase\n",
        "            # the elif condition will also match\n",
        "            # and we should avoid that\n",
        "            if self.re_other_name_for_match.match(s):\n",
        "                page_other_names += self.get_other_name(s)\n",
        "            elif self.re_name.match(s):\n",
        "                page_names += self.get_name(s)\n",
        "            elif self.re_house_num_for_match.match(s):\n",
        "                page_house_numbers += self.get_house_num(s)\n",
        "            elif self.re_age_gender_for_match.match(s):\n",
        "                ages, genders = self.get_age_gender(s)\n",
        "                page_ages += ages\n",
        "                page_genders += genders\n",
        "            elif self.re_age_for_match.match(s):\n",
        "                page_ages += self.get_age_only()\n",
        "            else:\n",
        "                # print(f\"DIDN not match: |{s}|\")\n",
        "                pass\n",
        "\n",
        "\n",
        "        # Ensure that we have the same number of rows in each column\n",
        "        check_array = [len(page_other_names), len(page_names)]\n",
        "        check_array += [len(page_house_numbers), len(page_genders)]\n",
        "        check_array += [len(page_ages)]\n",
        "        # print(check_array)\n",
        "        for i in range(len(check_array) - 1):\n",
        "            x = check_array[i]\n",
        "            for y in check_array[i:]:\n",
        "                assert(min(x, 30) == min(y, 30))\n",
        "\n",
        "        ret_array = []\n",
        "        for name, o_name, housenum, gender, age in \\\n",
        "            zip(\\\n",
        "                page_names,\\\n",
        "                page_other_names,\\\n",
        "                page_house_numbers,\\\n",
        "                page_genders,\\\n",
        "                page_ages):\n",
        "            val = { \\\n",
        "                \"name\": name,\\\n",
        "                \"other_name\": o_name,\\\n",
        "                \"house_num\": housenum,\\\n",
        "                \"gender\": gender,\\\n",
        "                \"age\": age,\\\n",
        "                \"town\": self.town,\\\n",
        "                \"block\": self.block,\\\n",
        "                \"post_office\": self.post_office,\\\n",
        "                \"police_station\": self.police_station,\\\n",
        "                \"pin_code\": self.pin_code, \\\n",
        "                \"page_number\": pagenum + 1,\\\n",
        "                \"url\": self.pdf_url,\\\n",
        "            }\n",
        "            ret_array.append(val)\n",
        "\n",
        "        return ret_array\n",
        "            \n",
        "        \n",
        "\n",
        "    def process(self)->pd.DataFrame:\n",
        "        if not os.path.isfile(self.temp_file_name) or \\\n",
        "            0 == os.stat(self.temp_file_name).st_size:\n",
        "            raise Exception(\"Failed to download file\")\n",
        "        self.pages = pdf2image.convert_from_path(self.temp_file_name)\n",
        "        #self.pages = self.pages[:10] + [self.pages[-1]]\n",
        "        description = f\"OCR Converting pages for {self.pdf_url}\"\n",
        "        self.pages_text = \\\n",
        "            {i: self.convert_to_text(i) for i in \\\n",
        "                tqdm.tqdm(range(len(self.pages)), desc=description)}\n",
        "        self.parse_first_page()\n",
        "        print(f\"Parsing {len(self.pages)} pages\")\n",
        "\n",
        "        for i in tqdm.tqdm(range(3, len(self.pages) - 1), desc=\"Parsing: \"):\n",
        "            try:\n",
        "                self.voters += self.parse_roll_page(i)\n",
        "            except Exception as e:\n",
        "                print()\n",
        "                print(f\"Error in processing page: {i + 1}\")\n",
        "                print(f\"of {self.pdf_url}\")\n",
        "                print(f\"Skipped the page, please verify manually\")\n",
        "                self.errors += f\"Error in processing page {i + 1}\"\n",
        "                self.errors += f\" of url: {self.pdf_url}\"\n",
        "                self.errors += f\". Skipped the page, please verify manually.\\n\"\n",
        "\n",
        "        temp_filename = self.get_temp_file_name() \n",
        "        with open(temp_filename, \"w\") as f:\n",
        "            json.dump(self.voters, f)\n",
        "        df = pd.read_json(temp_filename, orient=\"records\")\n",
        "\n",
        "        def get_last_name(x:str)->str:\n",
        "            return x.split()[-1]\n",
        "        df[\"name_lastname\"] = df[\"name\"].map(get_last_name)\n",
        "        df[\"other_name_lastname\"] = df[\"other_name\"].map(get_last_name)\n",
        "        self.processed_df = df\n",
        "        return df\n",
        "        \n",
        "    def save(self)->None:\n",
        "        if self.savedir and \"\" != self.savedir \\\n",
        "            and self.processed_df is not None:\n",
        "            save_filename = \\\n",
        "                f\"/content/gdrive/MyDrive/{self.savedir}/{self.save_filename}\"\n",
        "            save_filename_err = f\"{save_filename}.err.txt\"\n",
        "            save_dir = f\"/content/gdrive/MyDrive/{self.savedir}\"\n",
        "            try:\n",
        "                if os.path.exists(save_dir) and not os.path.isdir(save_dir):\n",
        "                    os.unlink(save_dir)\n",
        "                if not os.path.exists(save_dir):\n",
        "                    os.makedirs(\\\n",
        "                                f\"/content/gdrive/MyDrive/{self.savedir}\",\\\n",
        "                                exist_ok=True)\n",
        "                if self.errors != \"\":\n",
        "                    with open(save_filename_err, \"w\") as ferr:\n",
        "                        ferr.write(self.errors)\n",
        "                self.processed_df.to_csv(save_filename)\n",
        "            except Exception as e:\n",
        "                print(\"Failed to save the spreadsheet\")\n",
        "                print(e)\n",
        "            \n",
        "            for pagenum, pagetext in self.pages_text.items():\n",
        "                page_text_dir = f\"/content/gdrive/MyDrive/{self.savedir}/{self.save_filename}.pages\"\n",
        "                if os.path.exists(page_text_dir) and not os.path.isdir(page_text_dir):\n",
        "                    os.unlink(page_text_dir)\n",
        "                if not os.path.exists(page_text_dir):\n",
        "                    os.makedirs(page_text_dir, exist_ok=True)\n",
        "                save_filename = \\\n",
        "                    f\"{page_text_dir}/{self.save_filename}.{pagenum + 1}.txt\"\n",
        "                with open(save_filename, \"w\") as f:\n",
        "                    f.write(pagetext)\n",
        "                \n",
        "\n",
        "    def __del__(self):\n",
        "        if os.path.exists(self.temp_file_name):\n",
        "            os.unlink(self.temp_file_name)\n"
      ],
      "metadata": {
        "id": "REL4OKurSkC9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_url(roll_url:str, save_google_drive_directory:str)->tuple:\n",
        "    try:\n",
        "        drive.mount(\"/content/gdrive\")\n",
        "        roll = Roll(url=roll_url, save_directory=save_google_drive_directory)\n",
        "        roll.download()\n",
        "        df = roll.process()\n",
        "        roll.save()\n",
        "        return df.describe().T, df\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {roll_url} or in saving to {save_google_drive_directory}\")\n",
        "        print(e)\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "9PEJbSjHLqTM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start here"
      ],
      "metadata": {
        "id": "_LlE-UIXcTOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTRUCTIONS: MODIFY HERE\n",
        "# -------------------------\n",
        "# - Add the links to the PDFs below:\n",
        "# - Add between the box-brackets\n",
        "# - There should be no leading and trailing spces\n",
        "# - Each string should be within quotes (\"\")\n",
        "# - The terminating quote should be followed by a comma\n",
        "# - Best to add one URL in one line\n",
        "\n",
        "ONLINE_PDF_FILES_LIST = [\n",
        "    \"http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf\",\n",
        "]"
      ],
      "metadata": {
        "id": "0ux_Ro0tPBkC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTRUCTIONS: MODIFY HERE\n",
        "# -------------------------\n",
        "# - Files will be saved to google drive after processing\n",
        "# - You can specify the destination directory in your google drive\n",
        "# - If the directory does not exist, it will be created\n",
        "# - Right now I have specified \"DELETEME_Destionation_directory\"\n",
        "#   - Change it as you require, remember that the name should be within quotes\n",
        "#   - If the color coding changes, you know you've done something wrong\n",
        "#   - If you see a spell-check underline, you know you've done something wrong\n",
        "\n",
        "for url in tqdm.tqdm(ONLINE_PDF_FILES_LIST, desc=\"Processing URLs from ONLINE_PDF_FILES_LIST\"):\n",
        "    process_url(roll_url=url, save_google_drive_directory=\"DELETEME__Destination_directory\")   # <--- MODIFY HERE TO CHANGE THE DESTINATION DIRECTORY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqjXKW5_MqHP",
        "outputId": "4a740179-83c4-464c-b956-976d4b05f861"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing URLs from ONLINE_PDF_FILES_LIST:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:   4%|▍         | 1/26 [00:04<01:52,  4.51s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:   8%|▊         | 2/26 [00:06<01:11,  2.99s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  12%|█▏        | 3/26 [00:21<03:12,  8.37s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  15%|█▌        | 4/26 [00:33<03:40, 10.01s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  19%|█▉        | 5/26 [00:46<03:53, 11.10s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  23%|██▎       | 6/26 [00:59<03:55, 11.75s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  27%|██▋       | 7/26 [01:12<03:47, 11.98s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  31%|███       | 8/26 [01:23<03:32, 11.80s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  35%|███▍      | 9/26 [01:35<03:21, 11.85s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  38%|███▊      | 10/26 [01:47<03:11, 11.98s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  42%|████▏     | 11/26 [01:59<02:59, 11.96s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  46%|████▌     | 12/26 [02:11<02:47, 11.97s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  50%|█████     | 13/26 [02:24<02:38, 12.16s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  54%|█████▍    | 14/26 [02:33<02:15, 11.27s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  58%|█████▊    | 15/26 [02:45<02:04, 11.34s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  62%|██████▏   | 16/26 [02:57<01:56, 11.61s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  65%|██████▌   | 17/26 [03:09<01:46, 11.82s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  69%|██████▉   | 18/26 [03:22<01:36, 12.08s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  73%|███████▎  | 19/26 [03:35<01:25, 12.27s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  77%|███████▋  | 20/26 [03:47<01:13, 12.30s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  81%|████████  | 21/26 [03:59<01:00, 12.17s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  85%|████████▍ | 22/26 [04:10<00:47, 11.94s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  88%|████████▊ | 23/26 [04:22<00:36, 12.04s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  92%|█████████▏| 24/26 [04:27<00:19,  9.73s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf:  96%|█████████▌| 25/26 [04:32<00:08,  8.30s/it]\u001b[A\n",
            "OCR Converting pages for http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf: 100%|██████████| 26/26 [04:35<00:00, 10.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing 26 pages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Parsing: 100%|██████████| 22/22 [00:00<00:00, 694.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error in processing page: 4\n",
            "of http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf\n",
            "Skipped the page, please verify manually\n",
            "\n",
            "Error in processing page: 6\n",
            "of http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf\n",
            "Skipped the page, please verify manually\n",
            "\n",
            "Error in processing page: 8\n",
            "of http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf\n",
            "Skipped the page, please verify manually\n",
            "\n",
            "Error in processing page: 14\n",
            "of http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf\n",
            "Skipped the page, please verify manually\n",
            "\n",
            "Error in processing page: 24\n",
            "of http://ceomeghalaya.nic.in/erolls/pdf/english/A001/A0010002.pdf\n",
            "Skipped the page, please verify manually\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs from ONLINE_PDF_FILES_LIST: 100%|██████████| 1/1 [04:46<00:00, 286.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u5gClJYKXnzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}